{"cells":[{"cell_type":"markdown","metadata":{},"source":["Thanks to these great works:\n","  - [motono0223: js24-train-gbdt-model-with-lags-singlemodel](https://www.kaggle.com/code/motono0223/js24-train-gbdt-model-with-lags-singlemodel)\n","  - [yuanzhezhou: jane-street-baseline-lgb-xgb-and-catboost](https://www.kaggle.com/code/yuanzhezhou/jane-street-baseline-lgb-xgb-and-catboost)\n","\n","This notebook mainly includes the following contents:\n","\n","How to perform feature engineering with lagged N date_ids (utilizing shift and rolling operations)\n","How to dynamically store data of the most recent N date_ids for updating lag features\n","\n","The model employed herein is a single-fold XGBoost model incorporated with lag-1 (previous day) features, which achieved a score of 0.0052 on the Leaderboard (LB).\n","\n","When time_id=0 of each new date_id arrives, a lag label corresponding to the previous date_id of the current date_id is provided. For instance, when date_id=100, the label for date_id=99 is assigned; however, the date_id in this lag data remains 100 (to align with the current main dataset). This lag data needs to be combined with the dynamically stored most recent N days of data to construct additional shift and rolling features."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-01-12T16:00:53.257941Z","iopub.status.busy":"2025-01-12T16:00:53.257732Z","iopub.status.idle":"2025-01-12T16:00:57.711606Z","shell.execute_reply":"2025-01-12T16:00:57.710520Z","shell.execute_reply.started":"2025-01-12T16:00:53.257921Z"},"trusted":true},"outputs":[],"source":["!pip install rtdl_num_embeddings -q --no-index --find-links=/kaggle/input/jane-street-import/rtdl_num_embeddings"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-01-12T16:00:57.712962Z","iopub.status.busy":"2025-01-12T16:00:57.712620Z","iopub.status.idle":"2025-01-12T16:01:05.019551Z","shell.execute_reply":"2025-01-12T16:01:05.018880Z","shell.execute_reply.started":"2025-01-12T16:00:57.712940Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","\n","from sklearn.model_selection import train_test_split\n","\n","from sklearn.metrics import r2_score\n","import pandas as pd\n","import math\n","import numpy as np\n","from tqdm import tqdm\n","import polars as pl\n","from collections import OrderedDict\n","import sys\n","from tabm_reference import Model, make_parameter_groups\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import kaggle_evaluation.jane_street_inference_server\n","\n","import os\n","\n","import joblib\n","\n","from pytorch_lightning import LightningModule"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-01-12T16:01:05.020810Z","iopub.status.busy":"2025-01-12T16:01:05.020328Z","iopub.status.idle":"2025-01-12T16:01:05.030186Z","shell.execute_reply":"2025-01-12T16:01:05.029582Z","shell.execute_reply.started":"2025-01-12T16:01:05.020774Z"},"trusted":true},"outputs":[],"source":["feature_list = [f\"feature_{idx:02d}\" for idx in range(79) if idx != 61]\n","\n","target_col = \"responder_6\" \n","\n","feature_test = feature_list \\\n","                + [f\"responder_{idx}_lag_1\" for idx in range(9)] \n","\n","feature_cat = [\"feature_09\", \"feature_10\", \"feature_11\"]\n","feature_cont = [item for item in feature_test if item not in feature_cat]\n","\n","batch_size = 8192\n","\n","std_feature = [i for i in feature_list if i not in feature_cat] + [f\"responder_{idx}_lag_1\" for idx in range(9)]\n","\n","data_stats = joblib.load(\"/kaggle/input/my-own-js/data_stats.pkl\")\n","means = data_stats['mean']\n","stds = data_stats['std']\n","\n","def standardize(df, feature_cols, means, stds):\n","    return df.with_columns([\n","        ((pl.col(col) - means[col]) / stds[col]).alias(col) for col in feature_cols\n","    ])"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-01-12T16:01:05.032051Z","iopub.status.busy":"2025-01-12T16:01:05.031823Z","iopub.status.idle":"2025-01-12T16:01:05.043555Z","shell.execute_reply":"2025-01-12T16:01:05.042887Z","shell.execute_reply.started":"2025-01-12T16:01:05.032032Z"},"trusted":true},"outputs":[],"source":["category_mappings = {'feature_09': {2: 0, 4: 1, 9: 2, 11: 3, 12: 4, 14: 5, 15: 6, 25: 7, 26: 8, 30: 9, 34: 10, 42: 11, 44: 12, 46: 13, 49: 14, 50: 15, 57: 16, 64: 17, 68: 18, 70: 19, 81: 20, 82: 21},\n"," 'feature_10': {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 10: 7, 12: 8},\n"," 'feature_11': {9: 0, 11: 1, 13: 2, 16: 3, 24: 4, 25: 5, 34: 6, 40: 7, 48: 8, 50: 9, 59: 10, 62: 11, 63: 12, 66: 13,\n","  76: 14, 150: 15, 158: 16, 159: 17, 171: 18, 195: 19, 214: 20, 230: 21, 261: 22, 297: 23, 336: 24, 376: 25, 388: 26, 410: 27, 522: 28, 534: 29, 539: 30},\n"," 'symbol_id': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19,\n","  20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38},\n"," 'time_id' : {i : i for i in range(968)}}\n","\n","def encode_column(df, column, mapping):\n","    max_value = max(mapping.values())  \n","\n","    def encode_category(category):\n","        return mapping.get(category, max_value + 1)  \n","    \n","    return df.with_columns(\n","        pl.col(column).map_elements(encode_category).alias(column)\n","    )"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-01-12T16:01:05.045237Z","iopub.status.busy":"2025-01-12T16:01:05.044999Z","iopub.status.idle":"2025-01-12T16:01:05.819249Z","shell.execute_reply":"2025-01-12T16:01:05.818560Z","shell.execute_reply.started":"2025-01-12T16:01:05.045218Z"},"trusted":true},"outputs":[],"source":["class R2Loss(nn.Module):\n","    def __init__(self):\n","        super(R2Loss, self).__init__()\n","\n","    def forward(self, y_pred, y_true):\n","        mse_loss = torch.sum((y_pred - y_true) ** 2)\n","        var_y = torch.sum(y_true ** 2)\n","        loss = mse_loss / (var_y + 1e-38)\n","        return loss\n","\n","class NN(LightningModule):\n","    def __init__(self, n_cont_features, cat_cardinalities, n_classes, lr, weight_decay):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.k = 16\n","        self.model = Model(\n","                n_num_features=n_cont_features,\n","                cat_cardinalities=cat_cardinalities,\n","                n_classes=n_classes,\n","                backbone={\n","                    'type': 'MLP',\n","                    'n_blocks': 3 ,\n","                    'd_block': 512,\n","                    'dropout': 0.25,\n","                },\n","                bins=None,\n","                num_embeddings= None,\n","                arch_type='tabm',\n","                k=self.k,\n","            )\n","        self.lr = lr\n","        self.weight_decay = weight_decay\n","        self.training_step_outputs = []\n","        self.validation_step_outputs = []\n","        self.loss_fn = R2Loss()\n","        # self.loss_fn = weighted_mse_loss\n","\n","    def forward(self, x_cont, x_cat):\n","        return self.model(x_cont, x_cat).squeeze(-1)\n","\n","    def training_step(self, batch):\n","        x_cont,x_cat, y, w , w_y= batch\n","        x_cont = x_cont + torch.randn_like(x_cont) * 0.02\n","        y_hat = self(x_cont, x_cat)\n","        # loss = self.loss_fn(y_hat.flatten(0, 1), y.repeat_interleave(self.k), w_y.repeat_interleave(self.k))\n","        loss = self.loss_fn(y_hat.flatten(0, 1), y.repeat_interleave(self.k))\n","        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=x_cont.size(0))\n","        self.training_step_outputs.append((y_hat.mean(1), y, w))\n","        return loss\n","\n","    def validation_step(self, batch):\n","        x_cont,x_cat, y, w, w_y = batch\n","        x_cont = x_cont + torch.randn_like(x_cont) * 0.02\n","        y_hat = self(x_cont, x_cat)\n","        # loss = self.loss_fn(y_hat.flatten(0, 1), y.repeat_interleave(self.k), w_y.repeat_interleave(self.k))\n","        loss = self.loss_fn(y_hat.flatten(0, 1), y.repeat_interleave(self.k))\n","        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=x_cont.size(0))\n","        self.validation_step_outputs.append((y_hat.mean(1), y, w))\n","        return loss\n","\n","    def on_validation_epoch_end(self):\n","        \"\"\"Calculate validation WRMSE at the end of the epoch.\"\"\"\n","        y = torch.cat([x[1] for x in self.validation_step_outputs]).cpu().numpy()\n","        if self.trainer.sanity_checking:\n","            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n","        else:\n","            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n","            weights = torch.cat([x[2] for x in self.validation_step_outputs]).cpu().numpy()\n","            # r2_val\n","            val_r_square = r2_val(y, prob, weights)\n","            self.log(\"val_r_square\", val_r_square, prog_bar=True, on_step=False, on_epoch=True)\n","        self.validation_step_outputs.clear()\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.AdamW(make_parameter_groups(self.model), lr=self.lr, weight_decay=self.weight_decay)\n","        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5,\n","        #                                                        verbose=True)\n","        return {\n","            'optimizer': optimizer,\n","            # 'lr_scheduler': {\n","            #     'scheduler': scheduler,\n","            #     'monitor': 'val_r_square',\n","            # }\n","        }\n","\n","    def on_train_epoch_end(self):\n","        if self.trainer.sanity_checking:\n","            return\n","\n","        y = torch.cat([x[1] for x in self.training_step_outputs]).cpu().numpy()\n","        prob = torch.cat([x[0] for x in self.training_step_outputs]).detach().cpu().numpy()\n","        weights = torch.cat([x[2] for x in self.training_step_outputs]).cpu().numpy()\n","        # r2_training\n","        train_r_square = r2_val(y, prob, weights)\n","        self.log(\"train_r_square\", train_r_square, prog_bar=True, on_step=False, on_epoch=True)\n","        self.training_step_outputs.clear()\n","\n","        epoch = self.trainer.current_epoch\n","        metrics = {k: v.item() if isinstance(v, torch.Tensor) else v for k, v in self.trainer.logged_metrics.items()}\n","        formatted_metrics = {k: f\"{v:.5f}\" for k, v in metrics.items()}\n","        print(f\"Epoch {epoch}: {formatted_metrics}\")\n","        \n","class custom_args():\n","    def __init__(self):\n","        self.usegpu = True\n","        self.gpuid = 0\n","        self.seed = 42\n","        self.model = 'nn'\n","        self.use_wandb = False\n","        self.project = 'js-tabm-with-lags'\n","        self.dname = \"./input_df/\"\n","        self.loader_workers = 10   \n","        self.bs = 8192\n","        self.lr = 1e-3\n","        self.weight_decay = 8e-4\n","        self.n_cont_features = 84\n","        self.n_cat_features = 5\n","        self.n_classes = None\n","        self.cat_cardinalities = [23, 10, 32, 40, 969]\n","        self.patience = 7\n","        self.max_epochs = 10\n","        self.N_fold = 5\n","\n","\n","my_args = custom_args()\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","modelnn = NN.load_from_checkpoint('/kaggle/input/my-own-js/tabm_epochepoch03.ckpt').to(device)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2025-01-12T16:01:05.820451Z","iopub.status.busy":"2025-01-12T16:01:05.820128Z","iopub.status.idle":"2025-01-12T16:01:05.829890Z","shell.execute_reply":"2025-01-12T16:01:05.829192Z","shell.execute_reply.started":"2025-01-12T16:01:05.820423Z"},"trusted":true},"outputs":[],"source":["lags_ : pl.DataFrame | None = None\n","\n","lags_history = None\n","\n","def predictnn(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n","    global lags_, lags_history\n","    if lags is not None:\n","        lags_ = lags\n","    \n","    for col in feature_cat + ['symbol_id', 'time_id']:\n","        test = encode_column(test, col, category_mappings[col])\n","\n","    predictions = test.select(\n","        'row_id',\n","        pl.lit(0.0).alias('responder_6'),\n","    )\n","    \n","    symbol_ids = test.select('symbol_id').to_numpy()[:, 0]\n","\n","    time_id = test.select(\"time_id\").to_numpy()[0]\n","    timie_id_array = test.select(\"time_id\").to_numpy()[:, 0]\n","    \n","    \n","    if time_id == 0:\n","        lags = lags.with_columns(pl.col('time_id').cast(pl.Int64))\n","        lags = lags.with_columns(pl.col('symbol_id').cast(pl.Int64))\n","    \n","        lags_history = lags\n","        lags = lags.filter(pl.col(\"time_id\") == 0)\n","        \n","        \n","        test = test.join(lags, on=[\"time_id\", \"symbol_id\"],  how=\"left\")\n","    else:\n","        lags = lags_history.filter(pl.col(\"time_id\") == time_id)\n","        test = test.join(lags, on=[\"time_id\", \"symbol_id\"],  how=\"left\")\n","\n","    \n","    test = test.with_columns([\n","        pl.col(col).fill_null(0) for col in feature_list + [f\"responder_{idx}_lag_1\" for idx in range(9)] \n","    ])\n","\n","    test = standardize(test, std_feature, means, stds)\n","\n","\n","    X_test = test[feature_test].to_numpy()\n","    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n","\n","    symbol_tensor = torch.tensor(symbol_ids, dtype=torch.float32).to(device)\n","    time_tensor = torch.tensor(timie_id_array, dtype=torch.float32).to(device)\n","    X_cat = X_test_tensor[:, [9, 10, 11]]\n","    X_cont = X_test_tensor[:, [i for i in range(X_test_tensor.shape[1]) if i not in [9, 10, 11]]]\n","    # X_cont = X_cont + torch.randn_like(X_cont) * 0.02\n","\n","    X_cat = (torch.concat([X_cat, symbol_tensor.unsqueeze(-1), time_tensor.unsqueeze(-1)], axis=1)).to(torch.int64)\n","    \n","\n","    modelnn.eval()\n","    with torch.no_grad():\n","        \n","        outputs = modelnn(X_cont, X_cat)\n","        # Assuming the model outputs a tensor of shape (batch_size, 1)\n","        preds = outputs.squeeze(-1).cpu().numpy()\n","        preds = preds.mean(1)\n","    \n","    \n","    # predictions = \\\n","    # test.select('row_id').\\\n","    # with_columns(\n","    #     pl.Series(\n","    #         name   = 'responder_6', \n","    #         values = np.clip(preds, a_min = -5, a_max = 5),\n","    #         dtype  = pl.Float64,\n","    #     )\n","    # )\n","\n","\n","    # # The predict function must return a DataFrame\n","    # assert isinstance(predictions, pl.DataFrame | pd.DataFrame)\n","    # # with columns 'row_id', 'responer_6'\n","    # assert list(predictions.columns) == ['row_id', 'responder_6']\n","    # # and as many rows as the test data.\n","    # assert len(predictions) == len(test)\n","\n","    return preds"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2025-01-12T16:01:05.831003Z","iopub.status.busy":"2025-01-12T16:01:05.830741Z","iopub.status.idle":"2025-01-12T16:01:08.886139Z","shell.execute_reply":"2025-01-12T16:01:08.885205Z","shell.execute_reply.started":"2025-01-12T16:01:05.830983Z"},"trusted":true},"outputs":[{"data":{"text/plain":["polars.config.Config"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import polars as pl\n","import numpy as np\n","import os, gc\n","from tqdm.auto import tqdm\n","from matplotlib import pyplot as plt\n","import pickle\n","\n","from sklearn.metrics import r2_score\n","from lightgbm import LGBMRegressor\n","import lightgbm as lgb\n","from xgboost import XGBRegressor\n","import xgboost as xgb\n","from catboost import CatBoostRegressor\n","from sklearn.ensemble import VotingRegressor\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","pd.options.display.max_columns = None\n","\n","import kaggle_evaluation.jane_street_inference_server\n","\n","pl.Config.set_tbl_rows(100)\n","pl.Config.set_tbl_cols(400)\n","pl.Config.set_fmt_table_cell_list_len(5)"]},{"cell_type":"markdown","metadata":{},"source":["# Configurations"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2025-01-12T16:01:08.887802Z","iopub.status.busy":"2025-01-12T16:01:08.887082Z","iopub.status.idle":"2025-01-12T16:01:08.892290Z","shell.execute_reply":"2025-01-12T16:01:08.891246Z","shell.execute_reply.started":"2025-01-12T16:01:08.887769Z"},"trusted":true},"outputs":[],"source":["class CONFIG:\n","    debug = False\n","    seed = 42\n","    target_col = \"responder_6\"\n","    lag_cols_rename = { f\"responder_{idx}_lag_1\" : f\"responder_{idx}\" for idx in range(9)}\n","    lag_target_cols_name = [f\"responder_{idx}\" for idx in range(9)]\n","    lag_cols_original = [\"date_id\", \"time_id\", \"symbol_id\"] + [f\"responder_{idx}\" for idx in range(9)]\n","    model_path = \"/kaggle/input/janestreet-public-model/xgb_001.pkl\"\n","    lag_ndays = 4"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-01-12T16:01:08.893393Z","iopub.status.busy":"2025-01-12T16:01:08.893133Z","iopub.status.idle":"2025-01-12T16:01:08.906958Z","shell.execute_reply":"2025-01-12T16:01:08.906172Z","shell.execute_reply.started":"2025-01-12T16:01:08.893373Z"},"trusted":true},"outputs":[],"source":["def create_agg_list(day, columns):\n","    agg_mean_list = [pl.col(c).mean().name.suffix(f\"_mean_{day}d\") for c in columns]\n","    agg_std_list = [pl.col(c).std().name.suffix(f\"_std_{day}d\") for c in columns]\n","    agg_max_list = [pl.col(c).max().name.suffix(f\"_max_{day}d\") for c in columns]\n","    agg_last_list = [pl.col(c).last().name.suffix(f\"_last_{day}d\") for c in columns]\n","    agg_list = agg_mean_list + agg_std_list + agg_max_list + agg_last_list\n","    return agg_list"]},{"cell_type":"markdown","metadata":{},"source":["# Load model"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-01-12T16:01:08.908158Z","iopub.status.busy":"2025-01-12T16:01:08.907937Z","iopub.status.idle":"2025-01-12T16:01:08.950344Z","shell.execute_reply":"2025-01-12T16:01:08.949493Z","shell.execute_reply.started":"2025-01-12T16:01:08.908139Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["116\n"]}],"source":["with open( CONFIG.model_path, \"rb\") as fp:\n","    result = pickle.load(fp)\n","    \n","model = result[\"model\"]\n","features = result[\"features\"]\n","print(len(features))"]},{"cell_type":"markdown","metadata":{},"source":["Prior to performing inference on the test set, it is necessary to preliminarily construct historical data. The key considerations are as follows:\n","  - If lag features of N days are utilized, the last N days of data in the training set must first be stored.\n","  - The date_id in the test set starts from 0, so it is required to pre-adjust the date_ids of the training set and the test set to ensure consistency.\n","  - Currently, it is unclear whether the test set is immediately subsequent to the training set. Herein, we first adjust the date_ids of the training set to be consistent with those of the test set (i.e., -N, -N+1, ..., -2, -1)."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2025-01-12T16:01:08.951447Z","iopub.status.busy":"2025-01-12T16:01:08.951151Z","iopub.status.idle":"2025-01-12T16:01:09.314964Z","shell.execute_reply":"2025-01-12T16:01:09.314167Z","shell.execute_reply.started":"2025-01-12T16:01:08.951424Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div><style>\n",".dataframe > thead > tr,\n",".dataframe > tbody > tr {\n","  text-align: right;\n","  white-space: pre-wrap;\n","}\n","</style>\n","<small>shape: (5, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date_id</th><th>time_id</th><th>symbol_id</th><th>responder_0</th><th>responder_1</th><th>responder_2</th><th>responder_3</th><th>responder_4</th><th>responder_5</th><th>responder_6</th><th>responder_7</th><th>responder_8</th></tr><tr><td>i16</td><td>i16</td><td>i16</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>-1</td><td>967</td><td>34</td><td>0.501321</td><td>0.905332</td><td>-0.819582</td><td>-0.564046</td><td>-0.223018</td><td>-0.283954</td><td>-0.045938</td><td>0.009797</td><td>-0.102538</td></tr><tr><td>-1</td><td>967</td><td>35</td><td>-1.113053</td><td>0.69719</td><td>-1.619031</td><td>-1.222743</td><td>-0.706082</td><td>-0.291133</td><td>0.167733</td><td>0.099704</td><td>0.32461</td></tr><tr><td>-1</td><td>967</td><td>36</td><td>-1.019353</td><td>-0.460962</td><td>-2.026678</td><td>-0.848606</td><td>-0.305448</td><td>-1.256913</td><td>-0.109359</td><td>-0.027474</td><td>-0.253956</td></tr><tr><td>-1</td><td>967</td><td>37</td><td>0.23585</td><td>0.556479</td><td>0.618944</td><td>-0.243765</td><td>-0.108361</td><td>-0.260777</td><td>-0.486923</td><td>-0.275566</td><td>-1.020708</td></tr><tr><td>-1</td><td>967</td><td>38</td><td>0.542563</td><td>0.513193</td><td>0.814393</td><td>0.032767</td><td>0.025435</td><td>0.311465</td><td>-0.044797</td><td>0.011133</td><td>-0.0793</td></tr></tbody></table></div>"],"text/plain":["shape: (5, 12)\n","┌────────┬────────┬────────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐\n","│ date_i ┆ time_i ┆ symbol ┆ respo ┆ respo ┆ respo ┆ respo ┆ respo ┆ respo ┆ respo ┆ respo ┆ respo │\n","│ d      ┆ d      ┆ _id    ┆ nder_ ┆ nder_ ┆ nder_ ┆ nder_ ┆ nder_ ┆ nder_ ┆ nder_ ┆ nder_ ┆ nder_ │\n","│ ---    ┆ ---    ┆ ---    ┆ 0     ┆ 1     ┆ 2     ┆ 3     ┆ 4     ┆ 5     ┆ 6     ┆ 7     ┆ 8     │\n","│ i16    ┆ i16    ┆ i16    ┆ ---   ┆ ---   ┆ ---   ┆ ---   ┆ ---   ┆ ---   ┆ ---   ┆ ---   ┆ ---   │\n","│        ┆        ┆        ┆ f32   ┆ f32   ┆ f32   ┆ f32   ┆ f32   ┆ f32   ┆ f32   ┆ f32   ┆ f32   │\n","╞════════╪════════╪════════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╡\n","│ -1     ┆ 967    ┆ 34     ┆ 0.501 ┆ 0.905 ┆ -0.81 ┆ -0.56 ┆ -0.22 ┆ -0.28 ┆ -0.04 ┆ 0.009 ┆ -0.10 │\n","│        ┆        ┆        ┆ 321   ┆ 332   ┆ 9582  ┆ 4046  ┆ 3018  ┆ 3954  ┆ 5938  ┆ 797   ┆ 2538  │\n","│ -1     ┆ 967    ┆ 35     ┆ -1.11 ┆ 0.697 ┆ -1.61 ┆ -1.22 ┆ -0.70 ┆ -0.29 ┆ 0.167 ┆ 0.099 ┆ 0.324 │\n","│        ┆        ┆        ┆ 3053  ┆ 19    ┆ 9031  ┆ 2743  ┆ 6082  ┆ 1133  ┆ 733   ┆ 704   ┆ 61    │\n","│ -1     ┆ 967    ┆ 36     ┆ -1.01 ┆ -0.46 ┆ -2.02 ┆ -0.84 ┆ -0.30 ┆ -1.25 ┆ -0.10 ┆ -0.02 ┆ -0.25 │\n","│        ┆        ┆        ┆ 9353  ┆ 0962  ┆ 6678  ┆ 8606  ┆ 5448  ┆ 6913  ┆ 9359  ┆ 7474  ┆ 3956  │\n","│ -1     ┆ 967    ┆ 37     ┆ 0.235 ┆ 0.556 ┆ 0.618 ┆ -0.24 ┆ -0.10 ┆ -0.26 ┆ -0.48 ┆ -0.27 ┆ -1.02 │\n","│        ┆        ┆        ┆ 85    ┆ 479   ┆ 944   ┆ 3765  ┆ 8361  ┆ 0777  ┆ 6923  ┆ 5566  ┆ 0708  │\n","│ -1     ┆ 967    ┆ 38     ┆ 0.542 ┆ 0.513 ┆ 0.814 ┆ 0.032 ┆ 0.025 ┆ 0.311 ┆ -0.04 ┆ 0.011 ┆ -0.07 │\n","│        ┆        ┆        ┆ 563   ┆ 193   ┆ 393   ┆ 767   ┆ 435   ┆ 465   ┆ 4797  ┆ 133   ┆ 93    │\n","└────────┴────────┴────────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["history = pl.scan_parquet(\n","    \"/kaggle/input/jane-street-realtime-marketdata-forecasting/train.parquet\"\n",").select(['date_id','time_id','symbol_id'] + [f\"responder_{idx}\" for idx in range(9)]).filter(\n","    (pl.col(\"date_id\")>=(1698 - CONFIG.lag_ndays))&(pl.col(\"date_id\")<1698)\n",")\n","# 这里将历史date_id变为从-N到-1, 假设test的date_id=0紧随train的date_id=1698,\n","# 在第一个batch给出的lags应该是date_id=1698的responser(但date_id给的0),\n","# 这样history中最后一个date_id=1697变为-1, 正好可以和推理时给的lags衔接上\n","history = history.with_columns(\n","    date_id = (pl.col(\"date_id\") - pl.lit(1698)).cast(pl.Int16)\n",")\n","history = history.collect()\n","\n","# 这里是为了统一特征的dtypes(polars在concat时如果dtype对不上会报错)\n","history_column_types = {\n","    'date_id': pl.Int16,\n","    'time_id': pl.Int16,\n","    'symbol_id': pl.Int16\n","}\n","feature_column_types = {}\n","for f in [f\"feature_{idx:02d}\" for idx in range(79)]:\n","    feature_column_types[f] = pl.Float32\n","\n","responder_column_types = {}\n","for f in [f\"responder_{idx}\" for idx in range(9)]:\n","    responder_column_types[f] = pl.Float32\n","\n","history = history.cast(history_column_types)\n","history = history.cast(responder_column_types)\n","history.tail()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2025-01-12T16:01:09.316062Z","iopub.status.busy":"2025-01-12T16:01:09.315823Z","iopub.status.idle":"2025-01-12T16:01:09.369445Z","shell.execute_reply":"2025-01-12T16:01:09.368744Z","shell.execute_reply.started":"2025-01-12T16:01:09.316044Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<lightgbm.basic.Booster at 0x7efc4e3e7040>"]},"metadata":{},"output_type":"display_data"}],"source":["feature_cols2 = [f\"feature_{idx:02d}\" for idx in range(79)]+ [f\"responder_{idx}_lag_1\" for idx in range(9)]\n","\n","xgb_model2 = None\n","xgb_model2 = lgb.Booster(model_file=\"/kaggle/input/11111111/lgbm_model_2.json\")\n","xgb_feature_cols = [\"symbol_id\", \"time_id\"] + feature_cols2\n","display(xgb_model2)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2025-01-12T16:01:09.372124Z","iopub.status.busy":"2025-01-12T16:01:09.371904Z","iopub.status.idle":"2025-01-12T16:01:09.384625Z","shell.execute_reply":"2025-01-12T16:01:09.383971Z","shell.execute_reply.started":"2025-01-12T16:01:09.372105Z"},"trusted":true},"outputs":[],"source":["# Custom R2 score calculation function\n","def r2_val(y_true, y_pred, sample_weight):\n","    r2 = 1 - np.average((y_pred - y_true) ** 2, weights=sample_weight) / (np.average((y_true) ** 2, weights=sample_weight) + 1e-38) # R2スコア計算\n","    return r2\n","\n","\n","class NN3(LightningModule):\n","    # Definition of the NN model\n","    def __init__(self, input_dim, hidden_dims, dropouts, lr, weight_decay):\n","        # ~Arguments~\n","        # input_dim: Dimension of the input layer\n","        # hidden_dims: Specify the dimensions of hidden layers as a list\n","        # dropouts: Argument to specify dropout rates for each hidden layer as a list\n","        # lr: Learning rate\n","        # weight_decay: Weight decay for regularization\n","        \n","        super().__init__() # Code to call the constructor of LightningModule\n","        self.save_hyperparameters() # Save the arguments passed to the constructor as hyperparameters\n","        layers = [] # Empty list to store each layer of the NN\n","        in_dim = input_dim\n","        \n","        for i, hidden_dim in enumerate(hidden_dims):\n","            layers.append(nn.BatchNorm1d(in_dim)) # Add batch normalization layer to the layers list\n","            if i > 0: # Execute only after the first hidden layer\n","                layers.append(nn.SiLU()) # Add SiLU activation function to the layers list\n","            if i < len(dropouts): # Conditional branch to apply dropout layer\n","                layers.append(nn.Dropout(dropouts[i])) # Add dropout layer to the layers list\n","            layers.append(nn.Linear(in_dim, hidden_dim)) # Add linear layer to the layers list\n","            # layers.append(nn.ReLU())\n","            in_dim = hidden_dim # Set the input dimension for the next layer\n","        layers.append(nn.Linear(in_dim, 1))  # Add linear layer from input dimension to output dimension 1 as the output layer to the layers list\n","        layers.append(nn.Tanh()) # Add Tanh as the activation function for the output layer to the layers list\n","        self.model = nn.Sequential(*layers) # Create an nn.Sequential model that executes layers stored in the list in order and store it in self.model\n","        self.lr = lr # Store the learning rate passed to the constructor in the self.lr attribute\n","        self.weight_decay = weight_decay # Store the weight decay passed to the constructor in the self.weight_decay attribute\n","        self.validation_step_outputs = [] # Initialize empty list to store outputs of the validation step\n","\n","    # Forward propagation of the NN model\n","    def forward(self, x):\n","        return 5 * self.model(x).squeeze(-1)  # Output is a 1-dimensional tensor\n","\n","    # Training step: Calculate loss function and record logs\n","    def training_step(self, batch):\n","        x, y, w = batch\n","        y_hat = self(x)\n","        loss = F.mse_loss(y_hat, y, reduction='none') * w  # Consider sample weights\n","        loss = loss.mean() # Average loss\n","        self.log('train_loss', loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n","        return loss\n","\n","    # Validation step: Calculate loss function and record logs\n","    def validation_step(self, batch):\n","        x, y, w = batch\n","        y_hat = self(x)\n","        loss = F.mse_loss(y_hat, y, reduction='none') * w\n","        loss = loss.mean()\n","        self.log('val_loss', loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n","        self.validation_step_outputs.append((y_hat, y, w))\n","        return loss\n","\n","    # Execute at the end of the validation epoch: Calculate custom R2 score and record logs\n","    def on_validation_epoch_end(self):\n","        \"\"\"Calculate validation WRMSE at the end of the epoch.\"\"\"\n","        y = torch.cat([x[1] for x in self.validation_step_outputs]).cpu().numpy()\n","        if self.trainer.sanity_checking:\n","            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n","        else:\n","            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n","            weights = torch.cat([x[2] for x in self.validation_step_outputs]).cpu().numpy()\n","            # r2_val\n","            val_r_square = r2_val(y, prob, weights)\n","            self.log(\"val_r_square\", val_r_square, prog_bar=True, on_step=False, on_epoch=True)\n","        self.validation_step_outputs.clear()\n","\n","    # Set up optimization algorithm and learning rate scheduler\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n","        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5,\n","                                                               verbose=True)\n","        return {\n","            'optimizer': optimizer,\n","            'lr_scheduler': {\n","                'scheduler': scheduler,\n","                'monitor': 'val_loss',\n","            }\n","        }\n","\n","    # Output logs of training result metrics at the end of the training epoch\n","    def on_train_epoch_end(self):\n","        if self.trainer.sanity_checking:\n","            return\n","        epoch = self.trainer.current_epoch\n","        metrics = {k: v.item() if isinstance(v, torch.Tensor) else v for k, v in self.trainer.logged_metrics.items()}\n","        formatted_metrics = {k: f\"{v:.5f}\" for k, v in metrics.items()}\n","        print(f\"Epoch {epoch}: {formatted_metrics}\")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2025-01-12T16:01:09.385882Z","iopub.status.busy":"2025-01-12T16:01:09.385633Z","iopub.status.idle":"2025-01-12T16:01:09.881442Z","shell.execute_reply":"2025-01-12T16:01:09.880750Z","shell.execute_reply.started":"2025-01-12T16:01:09.385862Z"},"trusted":true},"outputs":[],"source":["N_folds = 5 # Set the number of models\n","# Load Best Model\n","modelsnn = [] # Initialize list to store models\n","for fold in range(N_folds): # Perform iterative processing for each fold\n","    checkpoint_path = f\"/kaggle/input/js-xs-nn-trained-model/nn_{fold}.model\" # Create checkpoint file path for the model\n","    modelnn3 = NN3.load_from_checkpoint(checkpoint_path) # Load model from checkpoint file\n","    modelsnn.append(modelnn3.to(\"cuda:0\")) # Transfer model to GPU and add to the list"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2025-01-12T16:01:09.882413Z","iopub.status.busy":"2025-01-12T16:01:09.882200Z","iopub.status.idle":"2025-01-12T16:01:09.886064Z","shell.execute_reply":"2025-01-12T16:01:09.885192Z","shell.execute_reply.started":"2025-01-12T16:01:09.882393Z"},"trusted":true},"outputs":[],"source":["lags_ : pl.DataFrame | None = None"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2025-01-12T16:01:09.887083Z","iopub.status.busy":"2025-01-12T16:01:09.886803Z","iopub.status.idle":"2025-01-12T16:01:09.898151Z","shell.execute_reply":"2025-01-12T16:01:09.897390Z","shell.execute_reply.started":"2025-01-12T16:01:09.887063Z"},"trusted":true},"outputs":[],"source":["CONfeature_cols  = [f\"feature_{idx:02d}\" for idx in range(79)]+ [f\"responder_{idx}_lag_1\" for idx in range(9)]"]},{"cell_type":"code","execution_count":19,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-01-12T16:01:27.558249Z","iopub.status.busy":"2025-01-12T16:01:27.557922Z","iopub.status.idle":"2025-01-12T16:01:27.568784Z","shell.execute_reply":"2025-01-12T16:01:27.567928Z","shell.execute_reply.started":"2025-01-12T16:01:27.558224Z"},"papermill":{"duration":0.018344,"end_time":"2024-10-10T11:58:33.59684","exception":false,"start_time":"2024-10-10T11:58:33.578496","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["\n","def predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n","    global history\n","    global lags_infer\n","    global lags_\n","    if lags is not None:\n","        lags_ = lags\n","\n","    pdss=predictnn(test,lags)\n","    \n","    symbol_ids = test.select(\"symbol_id\").to_numpy()[:, 0]\n","    current_date = test.select(\"date_id\").to_numpy()[:, 0][0]\n","\n","    lags2 = lags_.clone().group_by([\"date_id\", \"symbol_id\"], maintain_order=True).last()\n","    X_test = test.join(lags2, on=[\"date_id\", \"symbol_id\"],  how=\"left\")\n","    preds2 = np.zeros((test.shape[0],))\n","    preds2 += xgb_model2.predict(X_test[xgb_feature_cols].to_numpy())*1.1\n","\n","\n","    test_input = X_test[CONfeature_cols].to_pandas()\n","    test_input = test_input.fillna(method = 'ffill').fillna(0)\n","    test_input = torch.FloatTensor(test_input.values).to(\"cuda:0\")\n","    preds_nno = np.zeros((X_test.shape[0],))\n","    with torch.no_grad():\n","        for i, nn_model3 in enumerate(modelsnn):\n","            nn_model3.eval()\n","            preds_nno += nn_model3(test_input).cpu().numpy()/len(modelsnn)\n","    \n","    if lags is not None:\n","        lags = lags.rename(CONFIG.lag_cols_rename)\n","        lags = lags.cast(history_column_types)\n","        lags = lags.cast(responder_column_types)\n","\n","        history = pl.concat([history, lags])\n","        \n","        history = history.filter(pl.col(\"date_id\") > (current_date - CONFIG.lag_ndays))\n","        agg_list = create_agg_list(1, CONFIG.lag_target_cols_name)\n","        shift_n_data = history.filter(pl.col(\"date_id\") == current_date)\n","        lags_infer = shift_n_data.group_by([\"date_id\", \"symbol_id\"], maintain_order=True).agg(agg_list)\n","  \n","    test = test.cast(history_column_types)\n","    test = test.cast(feature_column_types)\n","    X_test = test.join(lags_infer, on=[\"date_id\", \"symbol_id\"], how=\"left\")\n","    preds = np.zeros((X_test.shape[0],))\n","    preds += model.predict(X_test[features].to_numpy())*1.05\n","\n","    pds = preds*0.7 + preds2*0.3\n","    #pds = np.clip(pds, a_min=-5, a_max=5)\n","\n","    #print(pdss)\n","    pdss2=pds*0.55+pdss*0.35+preds_nno*0.2*1.1\n","    pdss2 = np.clip(pdss2, a_min=-5, a_max=5)\n","    \n","    predictions = (\n","        test.select('row_id').with_columns(\n","            pl.Series(name='responder_6', values=pdss2, dtype=pl.Float64)\n","        )\n","    )\n","\n","    assert isinstance(predictions, pl.DataFrame | pd.DataFrame)\n","    assert list(predictions.columns) == ['row_id', 'responder_6']\n","    assert len(predictions) == len(test)\n","\n","    return predictions"]},{"cell_type":"code","execution_count":20,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-01-12T16:01:28.767757Z","iopub.status.busy":"2025-01-12T16:01:28.767420Z","iopub.status.idle":"2025-01-12T16:01:28.820370Z","shell.execute_reply":"2025-01-12T16:01:28.819711Z","shell.execute_reply.started":"2025-01-12T16:01:28.767729Z"},"papermill":{"duration":2.225871,"end_time":"2024-10-10T11:58:35.830964","exception":false,"start_time":"2024-10-10T11:58:33.605093","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["inference_server = kaggle_evaluation.jane_street_inference_server.JSInferenceServer(predict)\n","\n","if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n","    inference_server.serve()\n","else:\n","    inference_server.run_local_gateway(\n","        (\n","            '/kaggle/input/jane-street-realtime-marketdata-forecasting/test.parquet',\n","            '/kaggle/input/jane-street-realtime-marketdata-forecasting/lags.parquet',\n","        )\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":9871156,"sourceId":84493,"sourceType":"competition"},{"datasetId":6006872,"sourceId":9801075,"sourceType":"datasetVersion"},{"datasetId":6010899,"sourceId":9806342,"sourceType":"datasetVersion"},{"datasetId":6035149,"sourceId":9838282,"sourceType":"datasetVersion"},{"datasetId":6297065,"sourceId":10253875,"sourceType":"datasetVersion"},{"datasetId":6357971,"sourceId":10294161,"sourceType":"datasetVersion"},{"datasetId":6410107,"sourceId":10351700,"sourceType":"datasetVersion"},{"datasetId":6406897,"sourceId":10400845,"sourceType":"datasetVersion"},{"sourceId":215616115,"sourceType":"kernelVersion"},{"sourceId":216017958,"sourceType":"kernelVersion"}],"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":7.594014,"end_time":"2024-10-10T11:58:36.355301","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-10-10T11:58:28.761287","version":"2.6.0"}},"nbformat":4,"nbformat_minor":4}
